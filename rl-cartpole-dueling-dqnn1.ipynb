{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Implementation of Dueling Network Architecture For RL\n# https://arxiv.org/pdf/1511.06581.pdf"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport gym","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import tanh\nfrom scipy.special import softmax\nimport pdb\nfrom collections import deque","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DQNNModel(object):\n    '''\n    Two layer neural net\n    input X\n    Z1 = dot(X, W1) + B1\n    Z2 = tanh(Z1)\n    \n    #Advantage\n    Z3_A = dot(Z2, W2_A) + B2_A\n    y_hat_a = Z3_A\n    \n    #Value\n    Z3_V = dot(Z2, W2_V) + B2_V\n    y_hat_v = Z3_V\n    \n    Loss(L) = 0.5 * ( (y_hat_a+y_hat_v) - (y_a+y_v) )**2\n    \n    dL/dy_hat_a = dlogp_a = y_hat_a + y_hat_v - y_a - y_v\n    dL/dy_hat_v = dlogp_v = y_hat_a + y_hat_v - y_a - y_v\n    \n    dy_hat_a/dZ3_A = 1\n    dZ3_A/dW2_A = Z2\n    dZ3_A/dB2_A = 1\n    \n    dZ3_A/dZ2 = W2_A\n    dZ2/dZ1 = 1 - Z2**2\n    dZ1/dW1 = X\n    dZ1/dB1 = 1\n    \n    dy_hat_v/dZ3_V = 1\n    dZ3_V/dW2_V = Z2\n    dZ3_V/dB2_V = 1\n    \n    dZ3_V/dZ2 = W2_V\n    dZ2/dZ1 = 1 - Z2**2\n    dZ1/dW1 = X\n    dZ1/dB1 = 1\n    \n    \n    \n    Using chain rule\n    \n    dL/dW2_A = (dlogp_a)*Z2\n    dL/dB2_A = dlogp_a\n    \n    dL/dW1 = (dlogp_a)*W2_A*(1-Z2**2)*X\n    (input,hidden) = (1,output)*(hidden,output)*(1,hidden)*(1,input)\n    dL/dB1 = (dlogp_a)*W2_A*(1-Z2**2)\n    \n    dL/dW2_V = (dlogp_v)*Z2\n    dL/dB2_V = dlogp_v\n    \n    dL/dW1 = (dlogp_v)*W2_V*(1-Z2**2)*X\n    dL/dB1 = (dlogp_v)*W2_V*(1-Z2**2)\n    \n    \n    '''\n    def __init__(self, ninput, nhidden, noutput):\n        self.ninput_ = ninput\n        self.nhidden_ = nhidden\n        self.noutput_ = noutput\n        #initialize model weights\n        #X=(1,input)\n        self.W1_ = np.random.randn(ninput, nhidden) / np.sqrt(nhidden)\n        self.B1_ = np.random.randn(1, nhidden) / np.sqrt(nhidden)\n        \n        self.W2_A_ = np.random.randn(nhidden, noutput) / np.sqrt(noutput)\n        self.B2_A_ = np.random.randn(1, noutput) / np.sqrt(noutput)\n        \n        self.W2_V_ = np.random.randn(nhidden, 1) / np.sqrt(nhidden)\n        self.B2_V_ = np.random.randn(1, 1)\n        \n        \n        self.dW1_cache_ = 0.\n        self.dB1_cache_ = 0.\n        self.dW2_A_cache_ = 0.\n        self.dB2_A_cache_ = 0.\n        self.dW2_V_cache_ = 0.\n        self.dB2_V_cache_ = 0.\n\n        \n        \n    def __str__(self):\n        return_str = \"\"\n        for i,j in self.__dict__.items():\n            return_str += str(i) + \" \" + str(j) + \"\\n\"  \n        return return_str\n    \n    def forward_adv(self, x):\n        #x=(1,input)\n        #W1=(input,hidden)\n        z1 = np.dot(x, self.W1_) + self.B1_ #(1,hidden)\n        z2 = tanh(z1) #(1,hidden)\n        \n        z3_a = np.dot(z2, self.W2_A_) + self.B2_A_ #(1,output)\n        #W2=(hidden, output)\n        y_hat_adv = z3_a #(1,output)\n        return y_hat_adv, z1, z2, z3_a, self.W1_, self.W2_A_\n\n    def forward_val(self, x):\n        #x=(1,input)\n        #W1=(input,hidden)\n        z1 = np.dot(x, self.W1_) + self.B1_ #(1,hidden)\n        z2 = tanh(z1) #(1,hidden)\n        \n        z3_v = np.dot(z2, self.W2_V_) + self.B2_V_ #(1,output)\n        #W2=(hidden, output)\n        y_hat_val = z3_v #(1,1)\n        return y_hat_val, z1, z2, z3_v, self.W1_, self.W2_V_\n    \n    \n    def loss(self, y, y_hat):\n        loss = 0.5 * (y_hat - y)**2\n        return loss\n    \n    def backward(self, x, y_a, y_v, y_hat_a, y_hat_v, z1, z2, z3_v, z3_a, w1, w2_a, w2_v, clip=True):\n        #pdb.set_trace()\n        #1.5 multiplier comes from the fact that we subtract mean from y_hat_v and we have 2 actions\n        dlogp_a = 1.5 * ( (y_hat_a+y_hat_v)-(y_a+y_v) )\n        \n        dlogp_v = 1.5 * ( (y_hat_v+y_hat_a).mean(axis=1)-(y_v+y_a).mean(axis=1) )\n        \n        dlogp_v = np.reshape(dlogp_v, y_v.shape)\n        \n        if clip:\n            np.clip(dlogp_a, -1., 1., out=dlogp_a)\n            np.clip(dlogp_v, -1., 1., out=dlogp_v)\n            #dlogp_v = max(-1., min(1.,dlogp_v))\n        \n        dB2_A = dlogp_a # (1, output)\n        dW2_A = np.dot(z2.T, dlogp_a) #(hidden, output)\n        \n        dB2_V = dlogp_v\n        dW2_V = np.dot(z2.T, dlogp_v)\n        \n        dB1_a = np.dot(dlogp_a, w2_a.T)*(1-(z2*z2)) #(1,hidden)\n        dW1_a = np.dot(x.T, dB1_a) #(input,hidden)\n\n        dB1_v = np.dot(dlogp_v, w2_v.T)*(1-(z2*z2)) #(1,hidden)\n        dW1_v = np.dot(x.T, dB1_v) #(input,hidden)\n        \n        dB1 = dB1_a + dB1_v\n        dW1 = dW1_a + dW1_v\n        \n        return dW1, dB1, dW2_A, dB2_A, dW2_V, dB2_V\n            \n    \n    def update_SGD(self, dW1, dB1, dW2_A, dB2_A, dW2_V, dB2_V, lr=1e-4):\n        self.W1_ -= lr*dW1\n        self.B1_ -= lr*dB1\n        \n        self.W2_A_ -= lr*dW2_A\n        self.B2_A_ -= lr*dB2_A\n\n        self.W2_V_ -= lr*dW2_V\n        self.B2_V_ -= lr*dB2_V\n        \n    \n    def put_from_model(self, model):\n        self.W1_ = model.W1_\n        self.B1_ = model.B1_\n        self.W2_A_ = model.W2_A_\n        self.B2_A_ = model.B2_A_\n        self.W2_V_ = model.W2_V_\n        self.B2_V_ = model.B2_V_\n\n       \n    def update_RMSProp(self, dW1, dB1, dW2_A, dB2_A, dW2_V, dB2_V, lr=1e-4):\n        decay_rate = 0.9\n        eps = 1e-4\n        \n        self.dW1_cache_ = decay_rate * self.dW1_cache_ + (1 - decay_rate) * (dW1)**2\n        self.W1_ += - lr * dW1 / (np.sqrt(self.dW1_cache_) + eps)\n\n        self.dB1_cache_ = decay_rate * self.dB1_cache_ + (1 - decay_rate) * (dB1)**2\n        self.B1_ += - lr * dB1 / (np.sqrt(self.dB1_cache_) + eps)\n\n        self.dW2_A_cache_ = decay_rate * self.dW2_A_cache_ + (1 - decay_rate) * (dW2_A)**2\n        self.W2_A_ += - lr * dW2_A / (np.sqrt(self.dW2_A_cache_) + eps)\n\n        self.dB2_A_cache_ = decay_rate * self.dB2_A_cache_ + (1 - decay_rate) * (dB2_A)**2\n        self.B2_A_ += - lr * dB2_A / (np.sqrt(self.dB2_A_cache_) + eps)        \n\n        self.dW2_V_cache_ = decay_rate * self.dW2_V_cache_ + (1 - decay_rate) * (dW2_V)**2\n        self.W2_V_ += - lr * dW2_V / (np.sqrt(self.dW2_V_cache_) + eps)\n\n        self.dB2_V_cache_ = decay_rate * self.dB2_V_cache_ + (1 - decay_rate) * (dB2_V)**2\n        self.B2_V_ += - lr * dB2_V / (np.sqrt(self.dB2_V_cache_) + eps)        \n        ","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env_name = 'CartPole-v1'\nenv = gym.make(env_name)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state = env.reset()\nINPUT_UNITS = state.shape[0]\nHIDDEN_UNITS = 100\nOUTPUT_UNITS = env.action_space.n\nmodel = DQNNModel(INPUT_UNITS, HIDDEN_UNITS, OUTPUT_UNITS)\ntarget_model = DQNNModel(INPUT_UNITS, HIDDEN_UNITS, OUTPUT_UNITS)\n","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_episodes = 12000\n\nexperience_replay_size = 250\nexperience_replay_queue_max = 10000\n\ncopy_model_after = 5\ncopy_model_counter = 0\n\ndisplay_after = 20\ndisplay_after_counter = 0\n\nepsilon = 1\nepsilon_min = 0.01\nepsilon_decay = 0.995\n\ntotal_steps = 0\ntotal_episode_steps_rs = []\nepisode_steps_rs = []\nbatch_loss_rs = []\n\nreplay_queue = deque(maxlen=experience_replay_queue_max)\n\nfor episode in range(num_episodes):\n    episode_steps = 0\n    while True:\n        state = state.reshape(1,-1)\n        y_hat_v, _, _, _, _, _ = model.forward_val(state)\n        y_hat_a, _, _, _, _, _ = model.forward_adv(state)\n        \n        y_hat = y_hat_v + (y_hat_a - y_hat_a.mean(axis=1))\n        action_values = y_hat\n        actions_sm = softmax(action_values)\n        \n        #exploration vs exploitation\n        if epsilon > random.random():\n            action = 0 if 0.5>random.random() else 1\n        else:\n            action = 0 if actions_sm[0][0]>random.random() else 1        \n        \n        new_state, reward, done, info = env.step(action)\n        episode_steps += 1\n        \n        replay_queue.append([state,action,reward-done,new_state,done])\n\n        state = new_state\n        total_steps += 1\n\n        if (total_steps%experience_replay_size == 0):\n            batch_loss = 0.\n            batch = random.sample(list(replay_queue), experience_replay_size)\n            \n            dW1, dB1, dW2, dB2 = 0.,0.,0.,0.\n            for i,obs in enumerate(batch):\n                s, a, r, n_s, d = obs\n                \n                n_s = n_s.reshape(1,-1)\n                \n                y_a = np.zeros((1,env.action_space.n))\n                \n                if d:\n                    y_a[0][a] = r\n                    y_v = np.array([[0.]])\n                    \n                else:\n                    y_a[0][a] = r\n\n                    t_val_v,_,_,_,_,_ = target_model.forward_val(n_s)\n                    t_val_a,_,_,_,_,_ = target_model.forward_adv(n_s)\n                    \n                    t_val_a -= t_val_a.mean(axis=1)\n                    \n                    y_a += 0.99 * (t_val_a)\n                    y_v = 0.99*t_val_v\n                    \n\n                y_hat_v, z1, z2, z3_v, w1, w2_v = model.forward_val(s)\n                                \n                y_hat_a, z1, z2, z3_a, w1, w2_a = model.forward_adv(s)\n                y_hat_a -= y_hat_a.mean(axis=1)\n                \n                #pdb.set_trace()\n                dw1, db1, dw2_a, db2_a, dw2_v, db2_v = model.backward(s, y_a,y_v, y_hat_a, y_hat_v, z1, z2, z3_v, z3_a, w1, w2_a, w2_v)\n                                \n                #dW1 += dw1\n                #dB1 += db1\n                #dW2 += dw2\n                #dB2 += db2\n                model.update_SGD(dw1, db1, dw2_a, db2_a, dw2_v, db2_v)\n                #model.update_RMSProp(dw1, db1, dw2_a, db2_a, dw2_v, db2_v)\n                batch_loss += model.loss(y_a+y_v, y_hat_a+y_hat_v).mean()\n            \n            \n            #model.update_SGD(dW1, dB1, dW2, dB2)\n            #update target model with model\n            copy_model_counter += 1\n            if(copy_model_counter%copy_model_after == 0):\n                target_model.put_from_model(model)\n            \n            average_loss = batch_loss / len(batch)                         \n            batch_loss_rs.append(average_loss)\n            display_after_counter += 1\n\n            if epsilon > epsilon_min:\n                epsilon *= epsilon_decay\n            \n            if (display_after_counter%display_after == 0):\n                episode_steps_buffer = episode_steps_rs\n                episode_loss_buffer = batch_loss_rs[-display_after:]\n                #print(episode_loss_buffer)\n                min_steps = np.min(episode_steps_buffer)\n                max_steps = np.max(episode_steps_buffer)\n                avg_steps = np.mean(episode_steps_buffer)\n                print(\"Episode %d, batch episodes %d, max %d, min %d, avg %d steps, avg loss %f\"%(episode+1, len(episode_steps_rs),max_steps, \n                                                                                                  min_steps, avg_steps, np.mean(episode_loss_buffer)))\n                episode_steps_rs = []\n                                \n        if done:\n            state = env.reset()\n            episode_steps_rs.append(episode_steps)\n            total_episode_steps_rs.append(episode_steps)\n            break\n            ","execution_count":null,"outputs":[{"output_type":"stream","text":"Episode 223, batch episodes 222, max 71, min 9, avg 22 steps, avg loss 0.344705\nEpisode 437, batch episodes 214, max 70, min 8, avg 23 steps, avg loss 0.741256\nEpisode 695, batch episodes 258, max 99, min 9, avg 19 steps, avg loss 3.254281\nEpisode 995, batch episodes 300, max 52, min 8, avg 16 steps, avg loss 9.829246\nEpisode 1296, batch episodes 301, max 47, min 8, avg 16 steps, avg loss 21.273506\nEpisode 1648, batch episodes 352, max 37, min 8, avg 14 steps, avg loss 37.100487\nEpisode 2004, batch episodes 356, max 44, min 8, avg 14 steps, avg loss 60.907460\nEpisode 2385, batch episodes 381, max 44, min 8, avg 13 steps, avg loss 69.874365\nEpisode 2774, batch episodes 389, max 32, min 8, avg 12 steps, avg loss 25.581501\nEpisode 3136, batch episodes 362, max 36, min 8, avg 13 steps, avg loss 20.004325\nEpisode 3323, batch episodes 187, max 82, min 12, avg 26 steps, avg loss 27.993838\nEpisode 3424, batch episodes 101, max 153, min 16, avg 49 steps, avg loss 19.351142\nEpisode 3516, batch episodes 92, max 179, min 17, avg 54 steps, avg loss 9.530743\nEpisode 3614, batch episodes 98, max 167, min 19, avg 50 steps, avg loss 3.717287\nEpisode 3881, batch episodes 267, max 75, min 11, avg 18 steps, avg loss 3.211623\nEpisode 4073, batch episodes 192, max 76, min 9, avg 25 steps, avg loss 5.719453\nEpisode 4166, batch episodes 93, max 116, min 20, avg 53 steps, avg loss 7.957153\nEpisode 4366, batch episodes 200, max 91, min 12, avg 24 steps, avg loss 8.677517\nEpisode 4487, batch episodes 121, max 182, min 12, avg 41 steps, avg loss 12.706508\nEpisode 4603, batch episodes 116, max 159, min 12, avg 43 steps, avg loss 13.589985\nEpisode 4719, batch episodes 116, max 113, min 14, avg 43 steps, avg loss 13.682621\nEpisode 4809, batch episodes 90, max 133, min 18, avg 55 steps, avg loss 15.830726\nEpisode 4886, batch episodes 77, max 182, min 10, avg 64 steps, avg loss 14.652392\nEpisode 4951, batch episodes 65, max 153, min 16, avg 76 steps, avg loss 11.320556\nEpisode 5000, batch episodes 49, max 214, min 24, avg 103 steps, avg loss 14.501350\nEpisode 5040, batch episodes 40, max 335, min 29, avg 122 steps, avg loss 12.559544\nEpisode 5070, batch episodes 30, max 369, min 72, avg 163 steps, avg loss 7.234453\nEpisode 5095, batch episodes 25, max 474, min 24, avg 205 steps, avg loss 7.593537\nEpisode 5125, batch episodes 30, max 299, min 46, avg 163 steps, avg loss 4.656754\nEpisode 5151, batch episodes 26, max 500, min 55, avg 190 steps, avg loss 6.503327\nEpisode 5176, batch episodes 25, max 404, min 89, avg 201 steps, avg loss 5.956135\nEpisode 5202, batch episodes 26, max 461, min 43, avg 195 steps, avg loss 4.925255\nEpisode 5227, batch episodes 25, max 500, min 31, avg 200 steps, avg loss 4.995673\nEpisode 5249, batch episodes 22, max 397, min 91, avg 224 steps, avg loss 5.804810\nEpisode 5271, batch episodes 22, max 359, min 133, avg 226 steps, avg loss 6.228672\nEpisode 5290, batch episodes 19, max 434, min 109, avg 265 steps, avg loss 5.129517\nEpisode 5315, batch episodes 25, max 319, min 85, avg 193 steps, avg loss 4.563843\nEpisode 5335, batch episodes 20, max 485, min 126, avg 259 steps, avg loss 5.778268\nEpisode 5354, batch episodes 19, max 408, min 150, avg 259 steps, avg loss 3.759146\nEpisode 5376, batch episodes 22, max 375, min 78, avg 227 steps, avg loss 6.141939\nEpisode 5394, batch episodes 18, max 500, min 82, avg 281 steps, avg loss 4.018675\nEpisode 5416, batch episodes 22, max 338, min 94, avg 232 steps, avg loss 5.411999\nEpisode 5432, batch episodes 16, max 500, min 107, avg 301 steps, avg loss 1.854492\nEpisode 5453, batch episodes 21, max 500, min 88, avg 240 steps, avg loss 3.750523\nEpisode 5470, batch episodes 17, max 500, min 107, avg 290 steps, avg loss 5.743875\nEpisode 5487, batch episodes 17, max 472, min 105, avg 301 steps, avg loss 3.562374\nEpisode 5502, batch episodes 15, max 500, min 155, avg 336 steps, avg loss 3.278789\nEpisode 5515, batch episodes 13, max 500, min 100, avg 376 steps, avg loss 5.187950\nEpisode 5530, batch episodes 15, max 500, min 170, avg 338 steps, avg loss 3.601465\nEpisode 5544, batch episodes 14, max 500, min 87, avg 354 steps, avg loss 2.389432\nEpisode 5559, batch episodes 15, max 500, min 112, avg 317 steps, avg loss 2.914199\nEpisode 5575, batch episodes 16, max 500, min 134, avg 326 steps, avg loss 3.182585\nEpisode 5587, batch episodes 12, max 500, min 210, avg 405 steps, avg loss 1.574227\nEpisode 5601, batch episodes 14, max 500, min 29, avg 354 steps, avg loss 2.955000\nEpisode 5617, batch episodes 16, max 463, min 117, avg 316 steps, avg loss 4.272131\nEpisode 5630, batch episodes 13, max 500, min 270, avg 393 steps, avg loss 2.700681\nEpisode 5644, batch episodes 14, max 500, min 128, avg 345 steps, avg loss 4.252943\nEpisode 5656, batch episodes 12, max 500, min 231, avg 427 steps, avg loss 3.159720\nEpisode 5670, batch episodes 14, max 500, min 142, avg 344 steps, avg loss 3.727610\nEpisode 5684, batch episodes 14, max 500, min 215, avg 369 steps, avg loss 4.774299\nEpisode 5697, batch episodes 13, max 500, min 120, avg 384 steps, avg loss 3.185646\nEpisode 5709, batch episodes 12, max 500, min 110, avg 402 steps, avg loss 3.411693\nEpisode 5722, batch episodes 13, max 500, min 246, avg 374 steps, avg loss 3.511734\nEpisode 5735, batch episodes 13, max 500, min 299, avg 408 steps, avg loss 2.400322\nEpisode 5747, batch episodes 12, max 500, min 170, avg 403 steps, avg loss 1.929104\nEpisode 5759, batch episodes 12, max 500, min 127, avg 416 steps, avg loss 2.341733\nEpisode 5771, batch episodes 12, max 500, min 287, avg 428 steps, avg loss 2.142360\nEpisode 5782, batch episodes 11, max 500, min 316, avg 446 steps, avg loss 3.944214\nEpisode 5794, batch episodes 12, max 500, min 356, avg 418 steps, avg loss 2.936576\nEpisode 5805, batch episodes 11, max 500, min 340, avg 434 steps, avg loss 2.371754\nEpisode 5816, batch episodes 11, max 500, min 372, avg 474 steps, avg loss 3.585964\nEpisode 5828, batch episodes 12, max 500, min 327, avg 410 steps, avg loss 2.341099\nEpisode 5839, batch episodes 11, max 500, min 351, avg 445 steps, avg loss 2.619023\nEpisode 5852, batch episodes 13, max 500, min 292, avg 409 steps, avg loss 2.617601\nEpisode 5862, batch episodes 10, max 500, min 365, avg 460 steps, avg loss 2.078713\nEpisode 5875, batch episodes 13, max 500, min 270, avg 405 steps, avg loss 3.541552\nEpisode 5887, batch episodes 12, max 500, min 248, avg 415 steps, avg loss 3.239535\nEpisode 5899, batch episodes 12, max 500, min 312, avg 425 steps, avg loss 1.830270\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(np.array(range(len(total_episode_steps_rs))), total_episode_steps_rs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ns1 = pd.Series(total_episode_steps_rs).rolling(20).mean()\ns1.dropna(inplace=True)\nplt.plot(s1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ns2 = pd.Series(total_episode_steps_rs).rolling(50).mean()\ns2.dropna(inplace=True)\nplt.plot(s2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}