{"cells":[{"metadata":{},"cell_type":"markdown","source":"Implementation of \"Playing Atari with Reinforcement Learning Mnih et al. arXiv:1312.5602v1\n# https://arxiv.org/pdf/1312.5602v1.pdf"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport gym","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import tanh\nfrom scipy.special import softmax\nimport pdb\nfrom collections import deque","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DQNNModel(object):\n    '''\n    Two layer neural net\n    input X\n    Z1 = dot(X, W1) + B1\n    Z2 = tanh(Z1)\n    Z3 = dot(Z2, W2) + B2\n    y_hat = Z3\n    \n    Loss(L) = 0.5 * (y_hat - y)**2\n    \n    dL/dy_hat = y_hat-y\n    dy_hat/dZ3 = y_hat-y\n    dZ3/dW2 = Z2\n    dZ3/dB2 = 1\n    dZ3/dZ2 = W2\n    dZ2/dZ1 = 1 - Z2^2\n    dZ1/dW1 = X\n    dZ1/dB1 = 1\n    \n    Using chain rule\n    \n    dL/dW2 = (y_hat-y)*Z2\n    dL/dB2 = (y_hat-y)\n    \n    dL/dW1 = (y_hat-y)*W2*(1-Z2^2)*X\n    (input,hidden) = (1,output)*(hidden,output)*(1,hidden)*(1,input)\n    \n    dL/dB1 = (y_hat-y)*W2*(1-Z2^2)\n    \n    \n    '''\n    def __init__(self, ninput, nhidden, noutput):\n        self.ninput_ = ninput\n        self.nhidden_ = nhidden\n        self.noutput_ = noutput\n        #initialize model weights\n        #X=(1,input)\n        self.W1_ = np.random.randn(ninput, nhidden) / np.sqrt(nhidden)\n        self.B1_ = np.random.randn(1, nhidden) / np.sqrt(nhidden)\n        self.W2_ = np.random.randn(nhidden, noutput) / np.sqrt(noutput)\n        self.B2_ = np.random.randn(1, noutput) / np.sqrt(noutput)\n        \n        self.dW1_cache_ = 0.\n        self.dB1_cache_ = 0.\n        self.dW2_cache_ = 0.\n        self.dB2_cache_ = 0.\n    \n    def __str__(self):\n        return_str = \"\"\n        for i,j in self.__dict__.items():\n            return_str += str(i) + \" \" + str(j) + \"\\n\"  \n        return return_str\n    \n    def forward(self, x):\n        #x=(1,input)\n        #W1=(input,hidden)\n        z1 = np.dot(x, self.W1_) + self.B1_ #(1,hidden)\n        z2 = tanh(z1) #(1,hidden)\n        z3 = np.dot(z2, self.W2_) + self.B2_ #(1,output)\n        #W2=(hidden, output)\n        y_hat = z3 #(1,output)\n        return y_hat, z1, z2, z3, self.W1_, self.W2_\n    \n    def loss(self, y, y_hat):\n        loss = 0.5 * (y_hat - y)**2\n        return loss\n    \n    def backward(self, x, y, y_hat, z1, z2, z3, w1, w2):\n        #pdb.set_trace()\n        dlogp = y_hat-y        \n        dB2 = dlogp # (1, output)\n        dW2 = np.dot(z2.T, dlogp) #(hidden, output)\n        \n        dB1 = np.dot(dlogp, w2.T)*(1-(z2*z2)) #(1,hidden)\n        dW1 = np.dot(x.T, dB1) #(input,hidden)\n        return dW1, dB1, dW2, dB2\n            \n    \n    def update_SGD(self, dW1, dB1, dW2, dB2, lr=1e-3):\n        self.W1_ -= lr*dW1\n        self.B1_ -= lr*dB1\n        self.W2_ -= lr*dW2\n        self.B2_ -= lr*dB2\n                ","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env_name = 'CartPole-v1'\nenv = gym.make(env_name)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state = env.reset()\nINPUT_UNITS = state.shape[0]\nHIDDEN_UNITS = 100\nOUTPUT_UNITS = 1\nmodels = []\n#we create one model per action \nfor i in range(env.action_space.n):\n    model = DQNNModel(INPUT_UNITS, HIDDEN_UNITS, OUTPUT_UNITS)\n    models.append(model)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_episodes = 10000\n\nexperience_replay_size = 1000\nexperience_replay_queue_max = 10000\n\ndisplay_after = 20\ndisplay_after_counter = 0\n\ntotal_steps = 0\ntotal_episode_steps_rs = []\nepisode_steps_rs = []\nbatch_loss_rs = []\n\nepsilon = 1\nepsilon_min = 0.01\nepsilon_decay = 0.995\n\nreplay_queue = deque(maxlen=experience_replay_queue_max)\n\nfor episode in range(num_episodes):\n    episode_steps = 0\n    while True:\n        state = state.reshape(1,-1)\n        \n        action_values = []\n        for model in models:\n            y_hat, _, _, _, _, _ = model.forward(state)\n            action_values.append(y_hat)\n                            \n        actions_sm = softmax(action_values)\n        \n        #exploration vs exploitation\n        if epsilon > random.random():\n            action = 0 if 0.5>random.random() else 1\n        else:\n            action = 0 if actions_sm[0]>random.random() else 1\n        \n        new_state, reward, done, info = env.step(action)\n        episode_steps += 1\n        \n        replay_queue.append([state,action,reward-done,new_state,done])\n\n        state = new_state\n        total_steps += 1\n\n        if (total_steps%experience_replay_size == 0):\n            batch_loss = 0.\n            batch = random.sample(list(replay_queue), experience_replay_size)\n            model_grad = {'dw1':0., 'db1':0., 'dw2':0., 'db2':0.}\n            model_grads = []\n            for i in range(len(models)):\n                model_grads.append(model_grad)\n            \n            for i,obs in enumerate(batch):\n                s, a, r, n_s, d = obs\n                n_s = n_s.reshape(1,-1)\n\n                if d:\n                    target = r\n                else:\n                    target_vals = []\n                    for model in models:\n                        t_val,_,_,_,_,_ = model.forward(n_s)\n                        target_vals.append(t_val)\n                    model = None\n                    target = r + 0.99 * max(target_vals)\n\n\n                y_hat, z1, z2, z3, w1, w2 = models[a].forward(s)\n                y = target\n                dw1, db1, dw2, db2 = models[a].backward(s, y, y_hat, z1, z2, z3, w1, w2)\n                #accumulate\n                model_grads[a]['dw1'] += dw1\n                model_grads[a]['db1'] += db1\n                model_grads[a]['dw2'] += dw2\n                model_grads[a]['db2'] += db2\n                \n                batch_loss += models[a].loss(y, y_hat)\n\n            #update model\n            for i, model in enumerate(models):\n                model.update_SGD(model_grads[i]['dw1'], model_grads[i]['db1'], model_grads[i]['dw2'], model_grads[i]['db2'], lr=1e-4)\n                \n            average_loss = batch_loss / len(batch)                         \n            batch_loss_rs.append(average_loss)\n            display_after_counter += 1\n            \n            if epsilon > epsilon_min:\n                epsilon *= epsilon_decay\n            \n            if (display_after_counter%display_after == 0):\n                episode_steps_buffer = episode_steps_rs\n                episode_loss_buffer = batch_loss_rs[-display_after:]\n                #print(episode_loss_buffer)\n                min_steps = np.min(episode_steps_buffer)\n                max_steps = np.max(episode_steps_buffer)\n                avg_steps = np.mean(episode_steps_buffer)\n                print(\"Episode %d, batch episodes %d, max %d, min %d, avg %d steps, avg loss %f\"%(episode+1, len(episode_steps_rs),max_steps, \n                                                                                                  min_steps, avg_steps, np.mean(episode_loss_buffer)))\n                episode_steps_rs = []\n                                \n        if done:\n            state = env.reset()\n            episode_steps_rs.append(episode_steps)\n            total_episode_steps_rs.append(episode_steps)\n            break\n            \n","execution_count":null,"outputs":[{"output_type":"stream","text":"Episode 844, batch episodes 843, max 86, min 8, avg 23 steps, avg loss 135.153612\nEpisode 1524, batch episodes 680, max 185, min 8, avg 29 steps, avg loss 28.420147\nEpisode 2112, batch episodes 588, max 132, min 8, avg 34 steps, avg loss 30.206123\nEpisode 2587, batch episodes 475, max 216, min 9, avg 42 steps, avg loss 34.429649\nEpisode 2938, batch episodes 351, max 191, min 9, avg 56 steps, avg loss 32.358351\nEpisode 3178, batch episodes 240, max 244, min 10, avg 83 steps, avg loss 40.350849\nEpisode 3380, batch episodes 202, max 390, min 12, avg 99 steps, avg loss 37.418711\nEpisode 3532, batch episodes 152, max 303, min 9, avg 131 steps, avg loss 30.819214\nEpisode 3668, batch episodes 136, max 399, min 11, avg 146 steps, avg loss 22.702351\nEpisode 3798, batch episodes 130, max 363, min 17, avg 153 steps, avg loss 14.583669\nEpisode 3893, batch episodes 95, max 363, min 47, avg 208 steps, avg loss 9.643365\nEpisode 4004, batch episodes 111, max 394, min 26, avg 183 steps, avg loss 7.357138\nEpisode 4109, batch episodes 105, max 478, min 14, avg 189 steps, avg loss 8.653934\nEpisode 4211, batch episodes 102, max 425, min 57, avg 197 steps, avg loss 5.059073\nEpisode 4298, batch episodes 87, max 429, min 20, avg 229 steps, avg loss 6.303256\nEpisode 4378, batch episodes 80, max 500, min 15, avg 248 steps, avg loss 9.088518\nEpisode 4438, batch episodes 60, max 500, min 103, avg 332 steps, avg loss 11.303054\nEpisode 4482, batch episodes 44, max 500, min 228, avg 455 steps, avg loss 7.569300\nEpisode 4526, batch episodes 44, max 500, min 86, avg 449 steps, avg loss 8.030630\nEpisode 4573, batch episodes 47, max 500, min 71, avg 423 steps, avg loss 9.805752\nEpisode 4617, batch episodes 44, max 500, min 107, avg 461 steps, avg loss 10.545101\nEpisode 4658, batch episodes 41, max 500, min 367, avg 482 steps, avg loss 10.433336\nEpisode 4699, batch episodes 41, max 500, min 141, avg 488 steps, avg loss 7.280081\nEpisode 4741, batch episodes 42, max 500, min 242, avg 480 steps, avg loss 9.337500\nEpisode 4787, batch episodes 46, max 500, min 172, avg 429 steps, avg loss 11.626073\nEpisode 4830, batch episodes 43, max 500, min 239, avg 472 steps, avg loss 6.459936\nEpisode 4871, batch episodes 41, max 500, min 197, avg 481 steps, avg loss 10.210419\nEpisode 4913, batch episodes 42, max 500, min 146, avg 480 steps, avg loss 11.664206\nEpisode 4953, batch episodes 40, max 500, min 500, avg 500 steps, avg loss 12.665700\nEpisode 4994, batch episodes 41, max 500, min 136, avg 486 steps, avg loss 10.771493\nEpisode 5035, batch episodes 41, max 500, min 187, avg 481 steps, avg loss 8.078061\nEpisode 5077, batch episodes 42, max 500, min 227, avg 478 steps, avg loss 8.420637\nEpisode 5122, batch episodes 45, max 500, min 37, avg 444 steps, avg loss 10.238959\nEpisode 5166, batch episodes 44, max 500, min 169, avg 462 steps, avg loss 10.765325\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(np.array(range(len(total_episode_steps_rs))), total_episode_steps_rs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ns1 = pd.Series(total_episode_steps_rs).rolling(20).mean()\ns1.dropna(inplace=True)\nplt.plot(s1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ns2 = pd.Series(total_episode_steps_rs).rolling(50).mean()\ns2.dropna(inplace=True)\nplt.plot(s2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}